{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 872,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011467889908256881,
      "grad_norm": 1.6012779474258423,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 5.2876,
      "step": 10
    },
    {
      "epoch": 0.022935779816513763,
      "grad_norm": 1.7910374402999878,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 5.2513,
      "step": 20
    },
    {
      "epoch": 0.034403669724770644,
      "grad_norm": 2.230764627456665,
      "learning_rate": 3e-06,
      "loss": 5.1953,
      "step": 30
    },
    {
      "epoch": 0.045871559633027525,
      "grad_norm": 1.80514395236969,
      "learning_rate": 4.000000000000001e-06,
      "loss": 5.3152,
      "step": 40
    },
    {
      "epoch": 0.05733944954128441,
      "grad_norm": 1.9809962511062622,
      "learning_rate": 5e-06,
      "loss": 5.1926,
      "step": 50
    },
    {
      "epoch": 0.06880733944954129,
      "grad_norm": 1.706945776939392,
      "learning_rate": 6e-06,
      "loss": 5.1525,
      "step": 60
    },
    {
      "epoch": 0.08027522935779817,
      "grad_norm": 2.0383293628692627,
      "learning_rate": 7.000000000000001e-06,
      "loss": 5.1144,
      "step": 70
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 1.907078742980957,
      "learning_rate": 8.000000000000001e-06,
      "loss": 5.126,
      "step": 80
    },
    {
      "epoch": 0.10321100917431193,
      "grad_norm": 2.3391668796539307,
      "learning_rate": 9e-06,
      "loss": 5.1359,
      "step": 90
    },
    {
      "epoch": 0.11467889908256881,
      "grad_norm": 2.2740159034729004,
      "learning_rate": 1e-05,
      "loss": 4.945,
      "step": 100
    },
    {
      "epoch": 0.12614678899082568,
      "grad_norm": 2.0219690799713135,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 4.9278,
      "step": 110
    },
    {
      "epoch": 0.13761467889908258,
      "grad_norm": 3.0784034729003906,
      "learning_rate": 1.2e-05,
      "loss": 4.9275,
      "step": 120
    },
    {
      "epoch": 0.14908256880733944,
      "grad_norm": 2.2664084434509277,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 4.7958,
      "step": 130
    },
    {
      "epoch": 0.16055045871559634,
      "grad_norm": 2.1781277656555176,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 4.6832,
      "step": 140
    },
    {
      "epoch": 0.1720183486238532,
      "grad_norm": 2.0742266178131104,
      "learning_rate": 1.5e-05,
      "loss": 4.5096,
      "step": 150
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 2.6343142986297607,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.4755,
      "step": 160
    },
    {
      "epoch": 0.19495412844036697,
      "grad_norm": 2.281205415725708,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 4.2564,
      "step": 170
    },
    {
      "epoch": 0.20642201834862386,
      "grad_norm": 2.6451683044433594,
      "learning_rate": 1.8e-05,
      "loss": 4.1908,
      "step": 180
    },
    {
      "epoch": 0.21788990825688073,
      "grad_norm": 2.1521029472351074,
      "learning_rate": 1.9e-05,
      "loss": 4.0976,
      "step": 190
    },
    {
      "epoch": 0.22935779816513763,
      "grad_norm": 1.8967604637145996,
      "learning_rate": 2e-05,
      "loss": 3.9964,
      "step": 200
    },
    {
      "epoch": 0.2408256880733945,
      "grad_norm": 1.5530107021331787,
      "learning_rate": 2.1e-05,
      "loss": 3.8774,
      "step": 210
    },
    {
      "epoch": 0.25229357798165136,
      "grad_norm": 2.0542891025543213,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 3.8188,
      "step": 220
    },
    {
      "epoch": 0.26376146788990823,
      "grad_norm": 1.484924077987671,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 3.764,
      "step": 230
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 1.8848305940628052,
      "learning_rate": 2.4e-05,
      "loss": 3.7351,
      "step": 240
    },
    {
      "epoch": 0.286697247706422,
      "grad_norm": 1.7106894254684448,
      "learning_rate": 2.5e-05,
      "loss": 3.6538,
      "step": 250
    },
    {
      "epoch": 0.2981651376146789,
      "grad_norm": 1.6689828634262085,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 3.631,
      "step": 260
    },
    {
      "epoch": 0.30963302752293576,
      "grad_norm": 2.482642412185669,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 3.5769,
      "step": 270
    },
    {
      "epoch": 0.3211009174311927,
      "grad_norm": 2.0163002014160156,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 3.5325,
      "step": 280
    },
    {
      "epoch": 0.33256880733944955,
      "grad_norm": 2.2885100841522217,
      "learning_rate": 2.9e-05,
      "loss": 3.4693,
      "step": 290
    },
    {
      "epoch": 0.3440366972477064,
      "grad_norm": 1.9217135906219482,
      "learning_rate": 3e-05,
      "loss": 3.5718,
      "step": 300
    },
    {
      "epoch": 0.3555045871559633,
      "grad_norm": 1.7391016483306885,
      "learning_rate": 3.1e-05,
      "loss": 3.562,
      "step": 310
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 1.6630473136901855,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 3.4561,
      "step": 320
    },
    {
      "epoch": 0.37844036697247707,
      "grad_norm": 2.013758897781372,
      "learning_rate": 3.3e-05,
      "loss": 3.4681,
      "step": 330
    },
    {
      "epoch": 0.38990825688073394,
      "grad_norm": 1.4579575061798096,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 3.5228,
      "step": 340
    },
    {
      "epoch": 0.4013761467889908,
      "grad_norm": 1.6768091917037964,
      "learning_rate": 3.5e-05,
      "loss": 3.5109,
      "step": 350
    },
    {
      "epoch": 0.41284403669724773,
      "grad_norm": 1.796359658241272,
      "learning_rate": 3.6e-05,
      "loss": 3.4758,
      "step": 360
    },
    {
      "epoch": 0.4243119266055046,
      "grad_norm": 2.186749219894409,
      "learning_rate": 3.7e-05,
      "loss": 3.4063,
      "step": 370
    },
    {
      "epoch": 0.43577981651376146,
      "grad_norm": 1.6012550592422485,
      "learning_rate": 3.8e-05,
      "loss": 3.482,
      "step": 380
    },
    {
      "epoch": 0.44724770642201833,
      "grad_norm": 1.8475292921066284,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 3.4149,
      "step": 390
    },
    {
      "epoch": 0.45871559633027525,
      "grad_norm": 1.9513678550720215,
      "learning_rate": 4e-05,
      "loss": 3.4733,
      "step": 400
    },
    {
      "epoch": 0.4701834862385321,
      "grad_norm": 1.5117771625518799,
      "learning_rate": 4.1e-05,
      "loss": 3.4845,
      "step": 410
    },
    {
      "epoch": 0.481651376146789,
      "grad_norm": 2.1154332160949707,
      "learning_rate": 4.2e-05,
      "loss": 3.4012,
      "step": 420
    },
    {
      "epoch": 0.49311926605504586,
      "grad_norm": 1.5727158784866333,
      "learning_rate": 4.3e-05,
      "loss": 3.3955,
      "step": 430
    },
    {
      "epoch": 0.5045871559633027,
      "grad_norm": 1.8218742609024048,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 3.3933,
      "step": 440
    },
    {
      "epoch": 0.5160550458715596,
      "grad_norm": 1.8545008897781372,
      "learning_rate": 4.5e-05,
      "loss": 3.4233,
      "step": 450
    },
    {
      "epoch": 0.5275229357798165,
      "grad_norm": 2.2305984497070312,
      "learning_rate": 4.600000000000001e-05,
      "loss": 3.3922,
      "step": 460
    },
    {
      "epoch": 0.5389908256880734,
      "grad_norm": 1.8118088245391846,
      "learning_rate": 4.7e-05,
      "loss": 3.4987,
      "step": 470
    },
    {
      "epoch": 0.5504587155963303,
      "grad_norm": 2.0730514526367188,
      "learning_rate": 4.8e-05,
      "loss": 3.4052,
      "step": 480
    },
    {
      "epoch": 0.5619266055045872,
      "grad_norm": 2.1049208641052246,
      "learning_rate": 4.9e-05,
      "loss": 3.4421,
      "step": 490
    },
    {
      "epoch": 0.573394495412844,
      "grad_norm": 1.6399824619293213,
      "learning_rate": 5e-05,
      "loss": 3.4398,
      "step": 500
    },
    {
      "epoch": 0.5848623853211009,
      "grad_norm": 2.0219526290893555,
      "learning_rate": 4.865591397849463e-05,
      "loss": 3.3388,
      "step": 510
    },
    {
      "epoch": 0.5963302752293578,
      "grad_norm": 2.367554187774658,
      "learning_rate": 4.731182795698925e-05,
      "loss": 3.431,
      "step": 520
    },
    {
      "epoch": 0.6077981651376146,
      "grad_norm": 1.8444942235946655,
      "learning_rate": 4.596774193548387e-05,
      "loss": 3.3352,
      "step": 530
    },
    {
      "epoch": 0.6192660550458715,
      "grad_norm": 2.299511671066284,
      "learning_rate": 4.4623655913978496e-05,
      "loss": 3.4013,
      "step": 540
    },
    {
      "epoch": 0.6307339449541285,
      "grad_norm": 1.5158443450927734,
      "learning_rate": 4.327956989247312e-05,
      "loss": 3.4219,
      "step": 550
    },
    {
      "epoch": 0.6422018348623854,
      "grad_norm": 1.552520513534546,
      "learning_rate": 4.1935483870967746e-05,
      "loss": 3.3697,
      "step": 560
    },
    {
      "epoch": 0.6536697247706422,
      "grad_norm": 2.5656797885894775,
      "learning_rate": 4.0591397849462364e-05,
      "loss": 3.4163,
      "step": 570
    },
    {
      "epoch": 0.6651376146788991,
      "grad_norm": 2.1648194789886475,
      "learning_rate": 3.924731182795699e-05,
      "loss": 3.3152,
      "step": 580
    },
    {
      "epoch": 0.676605504587156,
      "grad_norm": 1.5385857820510864,
      "learning_rate": 3.7903225806451614e-05,
      "loss": 3.374,
      "step": 590
    },
    {
      "epoch": 0.6880733944954128,
      "grad_norm": 1.6695020198822021,
      "learning_rate": 3.655913978494624e-05,
      "loss": 3.3746,
      "step": 600
    },
    {
      "epoch": 0.6995412844036697,
      "grad_norm": 1.6615536212921143,
      "learning_rate": 3.5215053763440864e-05,
      "loss": 3.4194,
      "step": 610
    },
    {
      "epoch": 0.7110091743119266,
      "grad_norm": 1.5869885683059692,
      "learning_rate": 3.387096774193548e-05,
      "loss": 3.4027,
      "step": 620
    },
    {
      "epoch": 0.7224770642201835,
      "grad_norm": 1.7874168157577515,
      "learning_rate": 3.252688172043011e-05,
      "loss": 3.4157,
      "step": 630
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 1.605763554573059,
      "learning_rate": 3.118279569892473e-05,
      "loss": 3.3984,
      "step": 640
    },
    {
      "epoch": 0.7454128440366973,
      "grad_norm": 1.592063069343567,
      "learning_rate": 2.9838709677419357e-05,
      "loss": 3.3954,
      "step": 650
    },
    {
      "epoch": 0.7568807339449541,
      "grad_norm": 2.073084592819214,
      "learning_rate": 2.8494623655913982e-05,
      "loss": 3.3893,
      "step": 660
    },
    {
      "epoch": 0.768348623853211,
      "grad_norm": 1.8711307048797607,
      "learning_rate": 2.71505376344086e-05,
      "loss": 3.3449,
      "step": 670
    },
    {
      "epoch": 0.7798165137614679,
      "grad_norm": 1.2099239826202393,
      "learning_rate": 2.5806451612903226e-05,
      "loss": 3.3588,
      "step": 680
    },
    {
      "epoch": 0.7912844036697247,
      "grad_norm": 1.7748239040374756,
      "learning_rate": 2.446236559139785e-05,
      "loss": 3.4213,
      "step": 690
    },
    {
      "epoch": 0.8027522935779816,
      "grad_norm": 2.088862895965576,
      "learning_rate": 2.3118279569892472e-05,
      "loss": 3.3598,
      "step": 700
    },
    {
      "epoch": 0.8142201834862385,
      "grad_norm": 2.0714528560638428,
      "learning_rate": 2.1774193548387097e-05,
      "loss": 3.4116,
      "step": 710
    },
    {
      "epoch": 0.8256880733944955,
      "grad_norm": 2.2333085536956787,
      "learning_rate": 2.0430107526881722e-05,
      "loss": 3.4066,
      "step": 720
    },
    {
      "epoch": 0.8371559633027523,
      "grad_norm": 2.0869054794311523,
      "learning_rate": 1.9086021505376344e-05,
      "loss": 3.3894,
      "step": 730
    },
    {
      "epoch": 0.8486238532110092,
      "grad_norm": 1.9799833297729492,
      "learning_rate": 1.774193548387097e-05,
      "loss": 3.4168,
      "step": 740
    },
    {
      "epoch": 0.8600917431192661,
      "grad_norm": 2.104062795639038,
      "learning_rate": 1.639784946236559e-05,
      "loss": 3.3712,
      "step": 750
    },
    {
      "epoch": 0.8715596330275229,
      "grad_norm": 2.0216360092163086,
      "learning_rate": 1.5053763440860215e-05,
      "loss": 3.3665,
      "step": 760
    },
    {
      "epoch": 0.8830275229357798,
      "grad_norm": 1.6786290407180786,
      "learning_rate": 1.3709677419354839e-05,
      "loss": 3.3798,
      "step": 770
    },
    {
      "epoch": 0.8944954128440367,
      "grad_norm": 1.917476773262024,
      "learning_rate": 1.2365591397849464e-05,
      "loss": 3.3906,
      "step": 780
    },
    {
      "epoch": 0.9059633027522935,
      "grad_norm": 1.7196511030197144,
      "learning_rate": 1.1021505376344087e-05,
      "loss": 3.3998,
      "step": 790
    },
    {
      "epoch": 0.9174311926605505,
      "grad_norm": 1.9231778383255005,
      "learning_rate": 9.67741935483871e-06,
      "loss": 3.3508,
      "step": 800
    },
    {
      "epoch": 0.9288990825688074,
      "grad_norm": 1.6722865104675293,
      "learning_rate": 8.333333333333334e-06,
      "loss": 3.396,
      "step": 810
    },
    {
      "epoch": 0.9403669724770642,
      "grad_norm": 1.5127201080322266,
      "learning_rate": 6.989247311827957e-06,
      "loss": 3.367,
      "step": 820
    },
    {
      "epoch": 0.9518348623853211,
      "grad_norm": 2.1600663661956787,
      "learning_rate": 5.64516129032258e-06,
      "loss": 3.3659,
      "step": 830
    },
    {
      "epoch": 0.963302752293578,
      "grad_norm": 1.848949909210205,
      "learning_rate": 4.3010752688172045e-06,
      "loss": 3.3096,
      "step": 840
    },
    {
      "epoch": 0.9747706422018348,
      "grad_norm": 1.570666790008545,
      "learning_rate": 2.9569892473118283e-06,
      "loss": 3.3556,
      "step": 850
    },
    {
      "epoch": 0.9862385321100917,
      "grad_norm": 1.5516828298568726,
      "learning_rate": 1.6129032258064516e-06,
      "loss": 3.3923,
      "step": 860
    },
    {
      "epoch": 0.9977064220183486,
      "grad_norm": 1.7319855690002441,
      "learning_rate": 2.688172043010753e-07,
      "loss": 3.403,
      "step": 870
    }
  ],
  "logging_steps": 10,
  "max_steps": 872,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 57455332392960.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
