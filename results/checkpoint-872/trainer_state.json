{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 872,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011467889908256881,
      "grad_norm": 1.1875879764556885,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 5.3324,
      "step": 10
    },
    {
      "epoch": 0.022935779816513763,
      "grad_norm": 1.4763915538787842,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 5.2744,
      "step": 20
    },
    {
      "epoch": 0.034403669724770644,
      "grad_norm": 1.564889907836914,
      "learning_rate": 3e-06,
      "loss": 5.2321,
      "step": 30
    },
    {
      "epoch": 0.045871559633027525,
      "grad_norm": 1.229731559753418,
      "learning_rate": 4.000000000000001e-06,
      "loss": 5.2796,
      "step": 40
    },
    {
      "epoch": 0.05733944954128441,
      "grad_norm": 1.445536732673645,
      "learning_rate": 5e-06,
      "loss": 5.2097,
      "step": 50
    },
    {
      "epoch": 0.06880733944954129,
      "grad_norm": 1.3221042156219482,
      "learning_rate": 6e-06,
      "loss": 5.2398,
      "step": 60
    },
    {
      "epoch": 0.08027522935779817,
      "grad_norm": 1.4882489442825317,
      "learning_rate": 7.000000000000001e-06,
      "loss": 5.1585,
      "step": 70
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 1.8251194953918457,
      "learning_rate": 8.000000000000001e-06,
      "loss": 5.1773,
      "step": 80
    },
    {
      "epoch": 0.10321100917431193,
      "grad_norm": 1.576721429824829,
      "learning_rate": 9e-06,
      "loss": 5.1825,
      "step": 90
    },
    {
      "epoch": 0.11467889908256881,
      "grad_norm": 1.8380422592163086,
      "learning_rate": 1e-05,
      "loss": 5.1264,
      "step": 100
    },
    {
      "epoch": 0.12614678899082568,
      "grad_norm": 1.5050958395004272,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 5.0819,
      "step": 110
    },
    {
      "epoch": 0.13761467889908258,
      "grad_norm": 1.7768861055374146,
      "learning_rate": 1.2e-05,
      "loss": 5.1842,
      "step": 120
    },
    {
      "epoch": 0.14908256880733944,
      "grad_norm": 1.440477967262268,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 5.027,
      "step": 130
    },
    {
      "epoch": 0.16055045871559634,
      "grad_norm": 1.95937979221344,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 5.0494,
      "step": 140
    },
    {
      "epoch": 0.1720183486238532,
      "grad_norm": 1.7060564756393433,
      "learning_rate": 1.5e-05,
      "loss": 4.9666,
      "step": 150
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 1.8483484983444214,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.9842,
      "step": 160
    },
    {
      "epoch": 0.19495412844036697,
      "grad_norm": 2.0981085300445557,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 4.7453,
      "step": 170
    },
    {
      "epoch": 0.20642201834862386,
      "grad_norm": 2.7163445949554443,
      "learning_rate": 1.8e-05,
      "loss": 4.8009,
      "step": 180
    },
    {
      "epoch": 0.21788990825688073,
      "grad_norm": 2.2853684425354004,
      "learning_rate": 1.9e-05,
      "loss": 4.7104,
      "step": 190
    },
    {
      "epoch": 0.22935779816513763,
      "grad_norm": 2.1746017932891846,
      "learning_rate": 2e-05,
      "loss": 4.6403,
      "step": 200
    },
    {
      "epoch": 0.2408256880733945,
      "grad_norm": 1.850457787513733,
      "learning_rate": 2.1e-05,
      "loss": 4.5593,
      "step": 210
    },
    {
      "epoch": 0.25229357798165136,
      "grad_norm": 1.8113209009170532,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 4.5478,
      "step": 220
    },
    {
      "epoch": 0.26376146788990823,
      "grad_norm": 1.780832290649414,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 4.3807,
      "step": 230
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 1.790887713432312,
      "learning_rate": 2.4e-05,
      "loss": 4.3721,
      "step": 240
    },
    {
      "epoch": 0.286697247706422,
      "grad_norm": 1.9532183408737183,
      "learning_rate": 2.5e-05,
      "loss": 4.2555,
      "step": 250
    },
    {
      "epoch": 0.2981651376146789,
      "grad_norm": 1.689055323600769,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 4.1765,
      "step": 260
    },
    {
      "epoch": 0.30963302752293576,
      "grad_norm": 2.499807596206665,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 4.0993,
      "step": 270
    },
    {
      "epoch": 0.3211009174311927,
      "grad_norm": 1.9469362497329712,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 4.0012,
      "step": 280
    },
    {
      "epoch": 0.33256880733944955,
      "grad_norm": 2.26131272315979,
      "learning_rate": 2.9e-05,
      "loss": 3.9825,
      "step": 290
    },
    {
      "epoch": 0.3440366972477064,
      "grad_norm": 1.74428129196167,
      "learning_rate": 3e-05,
      "loss": 3.9319,
      "step": 300
    },
    {
      "epoch": 0.3555045871559633,
      "grad_norm": 2.4550998210906982,
      "learning_rate": 3.1e-05,
      "loss": 3.8635,
      "step": 310
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 1.9020676612854004,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 3.7394,
      "step": 320
    },
    {
      "epoch": 0.37844036697247707,
      "grad_norm": 2.1913506984710693,
      "learning_rate": 3.3e-05,
      "loss": 3.7442,
      "step": 330
    },
    {
      "epoch": 0.38990825688073394,
      "grad_norm": 1.5352877378463745,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 3.7532,
      "step": 340
    },
    {
      "epoch": 0.4013761467889908,
      "grad_norm": 1.621175765991211,
      "learning_rate": 3.5e-05,
      "loss": 3.7113,
      "step": 350
    },
    {
      "epoch": 0.41284403669724773,
      "grad_norm": 1.71941339969635,
      "learning_rate": 3.6e-05,
      "loss": 3.6513,
      "step": 360
    },
    {
      "epoch": 0.4243119266055046,
      "grad_norm": 1.8082013130187988,
      "learning_rate": 3.7e-05,
      "loss": 3.5695,
      "step": 370
    },
    {
      "epoch": 0.43577981651376146,
      "grad_norm": 1.764923095703125,
      "learning_rate": 3.8e-05,
      "loss": 3.6366,
      "step": 380
    },
    {
      "epoch": 0.44724770642201833,
      "grad_norm": 2.036907434463501,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 3.5576,
      "step": 390
    },
    {
      "epoch": 0.45871559633027525,
      "grad_norm": 1.592660665512085,
      "learning_rate": 4e-05,
      "loss": 3.6198,
      "step": 400
    },
    {
      "epoch": 0.4701834862385321,
      "grad_norm": 1.6183651685714722,
      "learning_rate": 4.1e-05,
      "loss": 3.5928,
      "step": 410
    },
    {
      "epoch": 0.481651376146789,
      "grad_norm": 1.9273825883865356,
      "learning_rate": 4.2e-05,
      "loss": 3.524,
      "step": 420
    },
    {
      "epoch": 0.49311926605504586,
      "grad_norm": 1.583420991897583,
      "learning_rate": 4.3e-05,
      "loss": 3.5016,
      "step": 430
    },
    {
      "epoch": 0.5045871559633027,
      "grad_norm": 1.5682591199874878,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 3.5229,
      "step": 440
    },
    {
      "epoch": 0.5160550458715596,
      "grad_norm": 2.0175817012786865,
      "learning_rate": 4.5e-05,
      "loss": 3.5523,
      "step": 450
    },
    {
      "epoch": 0.5275229357798165,
      "grad_norm": 1.923862338066101,
      "learning_rate": 4.600000000000001e-05,
      "loss": 3.4872,
      "step": 460
    },
    {
      "epoch": 0.5389908256880734,
      "grad_norm": 1.9057544469833374,
      "learning_rate": 4.7e-05,
      "loss": 3.5839,
      "step": 470
    },
    {
      "epoch": 0.5504587155963303,
      "grad_norm": 1.9184908866882324,
      "learning_rate": 4.8e-05,
      "loss": 3.5103,
      "step": 480
    },
    {
      "epoch": 0.5619266055045872,
      "grad_norm": 1.7999058961868286,
      "learning_rate": 4.9e-05,
      "loss": 3.55,
      "step": 490
    },
    {
      "epoch": 0.573394495412844,
      "grad_norm": 1.5304068326950073,
      "learning_rate": 5e-05,
      "loss": 3.5179,
      "step": 500
    },
    {
      "epoch": 0.5848623853211009,
      "grad_norm": 1.8993282318115234,
      "learning_rate": 4.865591397849463e-05,
      "loss": 3.4634,
      "step": 510
    },
    {
      "epoch": 0.5963302752293578,
      "grad_norm": 1.7923029661178589,
      "learning_rate": 4.731182795698925e-05,
      "loss": 3.5317,
      "step": 520
    },
    {
      "epoch": 0.6077981651376146,
      "grad_norm": 1.7665650844573975,
      "learning_rate": 4.596774193548387e-05,
      "loss": 3.4272,
      "step": 530
    },
    {
      "epoch": 0.6192660550458715,
      "grad_norm": 1.777608871459961,
      "learning_rate": 4.4623655913978496e-05,
      "loss": 3.5192,
      "step": 540
    },
    {
      "epoch": 0.6307339449541285,
      "grad_norm": 2.0637047290802,
      "learning_rate": 4.327956989247312e-05,
      "loss": 3.4926,
      "step": 550
    },
    {
      "epoch": 0.6422018348623854,
      "grad_norm": 1.7028895616531372,
      "learning_rate": 4.1935483870967746e-05,
      "loss": 3.4579,
      "step": 560
    },
    {
      "epoch": 0.6536697247706422,
      "grad_norm": 2.677696466445923,
      "learning_rate": 4.0591397849462364e-05,
      "loss": 3.4683,
      "step": 570
    },
    {
      "epoch": 0.6651376146788991,
      "grad_norm": 2.2520477771759033,
      "learning_rate": 3.924731182795699e-05,
      "loss": 3.4155,
      "step": 580
    },
    {
      "epoch": 0.676605504587156,
      "grad_norm": 1.5822926759719849,
      "learning_rate": 3.7903225806451614e-05,
      "loss": 3.455,
      "step": 590
    },
    {
      "epoch": 0.6880733944954128,
      "grad_norm": 2.0215606689453125,
      "learning_rate": 3.655913978494624e-05,
      "loss": 3.4703,
      "step": 600
    },
    {
      "epoch": 0.6995412844036697,
      "grad_norm": 1.5860363245010376,
      "learning_rate": 3.5215053763440864e-05,
      "loss": 3.5139,
      "step": 610
    },
    {
      "epoch": 0.7110091743119266,
      "grad_norm": 1.5994993448257446,
      "learning_rate": 3.387096774193548e-05,
      "loss": 3.4577,
      "step": 620
    },
    {
      "epoch": 0.7224770642201835,
      "grad_norm": 1.888985514640808,
      "learning_rate": 3.252688172043011e-05,
      "loss": 3.5023,
      "step": 630
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 1.7887578010559082,
      "learning_rate": 3.118279569892473e-05,
      "loss": 3.4515,
      "step": 640
    },
    {
      "epoch": 0.7454128440366973,
      "grad_norm": 1.5872563123703003,
      "learning_rate": 2.9838709677419357e-05,
      "loss": 3.4181,
      "step": 650
    },
    {
      "epoch": 0.7568807339449541,
      "grad_norm": 2.0898540019989014,
      "learning_rate": 2.8494623655913982e-05,
      "loss": 3.4513,
      "step": 660
    },
    {
      "epoch": 0.768348623853211,
      "grad_norm": 1.9276561737060547,
      "learning_rate": 2.71505376344086e-05,
      "loss": 3.3906,
      "step": 670
    },
    {
      "epoch": 0.7798165137614679,
      "grad_norm": 1.2906289100646973,
      "learning_rate": 2.5806451612903226e-05,
      "loss": 3.4332,
      "step": 680
    },
    {
      "epoch": 0.7912844036697247,
      "grad_norm": 1.8818552494049072,
      "learning_rate": 2.446236559139785e-05,
      "loss": 3.4772,
      "step": 690
    },
    {
      "epoch": 0.8027522935779816,
      "grad_norm": 1.9695322513580322,
      "learning_rate": 2.3118279569892472e-05,
      "loss": 3.443,
      "step": 700
    },
    {
      "epoch": 0.8142201834862385,
      "grad_norm": 2.062731981277466,
      "learning_rate": 2.1774193548387097e-05,
      "loss": 3.4418,
      "step": 710
    },
    {
      "epoch": 0.8256880733944955,
      "grad_norm": 2.1845245361328125,
      "learning_rate": 2.0430107526881722e-05,
      "loss": 3.4607,
      "step": 720
    },
    {
      "epoch": 0.8371559633027523,
      "grad_norm": 2.0848796367645264,
      "learning_rate": 1.9086021505376344e-05,
      "loss": 3.4653,
      "step": 730
    },
    {
      "epoch": 0.8486238532110092,
      "grad_norm": 2.2559878826141357,
      "learning_rate": 1.774193548387097e-05,
      "loss": 3.4868,
      "step": 740
    },
    {
      "epoch": 0.8600917431192661,
      "grad_norm": 2.0856895446777344,
      "learning_rate": 1.639784946236559e-05,
      "loss": 3.4519,
      "step": 750
    },
    {
      "epoch": 0.8715596330275229,
      "grad_norm": 2.059861660003662,
      "learning_rate": 1.5053763440860215e-05,
      "loss": 3.421,
      "step": 760
    },
    {
      "epoch": 0.8830275229357798,
      "grad_norm": 1.5613343715667725,
      "learning_rate": 1.3709677419354839e-05,
      "loss": 3.4407,
      "step": 770
    },
    {
      "epoch": 0.8944954128440367,
      "grad_norm": 1.7479159832000732,
      "learning_rate": 1.2365591397849464e-05,
      "loss": 3.4329,
      "step": 780
    },
    {
      "epoch": 0.9059633027522935,
      "grad_norm": 1.6508941650390625,
      "learning_rate": 1.1021505376344087e-05,
      "loss": 3.4729,
      "step": 790
    },
    {
      "epoch": 0.9174311926605505,
      "grad_norm": 2.011017322540283,
      "learning_rate": 9.67741935483871e-06,
      "loss": 3.3989,
      "step": 800
    },
    {
      "epoch": 0.9288990825688074,
      "grad_norm": 1.7184510231018066,
      "learning_rate": 8.333333333333334e-06,
      "loss": 3.4609,
      "step": 810
    },
    {
      "epoch": 0.9403669724770642,
      "grad_norm": 1.421589970588684,
      "learning_rate": 6.989247311827957e-06,
      "loss": 3.4361,
      "step": 820
    },
    {
      "epoch": 0.9518348623853211,
      "grad_norm": 1.8659533262252808,
      "learning_rate": 5.64516129032258e-06,
      "loss": 3.4119,
      "step": 830
    },
    {
      "epoch": 0.963302752293578,
      "grad_norm": 1.7596635818481445,
      "learning_rate": 4.3010752688172045e-06,
      "loss": 3.3765,
      "step": 840
    },
    {
      "epoch": 0.9747706422018348,
      "grad_norm": 1.5556881427764893,
      "learning_rate": 2.9569892473118283e-06,
      "loss": 3.4195,
      "step": 850
    },
    {
      "epoch": 0.9862385321100917,
      "grad_norm": 1.8219952583312988,
      "learning_rate": 1.6129032258064516e-06,
      "loss": 3.4527,
      "step": 860
    },
    {
      "epoch": 0.9977064220183486,
      "grad_norm": 1.877001166343689,
      "learning_rate": 2.688172043010753e-07,
      "loss": 3.4639,
      "step": 870
    }
  ],
  "logging_steps": 10,
  "max_steps": 872,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 57110002237440.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
